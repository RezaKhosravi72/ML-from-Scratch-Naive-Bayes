{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542f6b3a-c592-4b4f-bd5b-67f1f92cb589",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier implementation:\n",
    "\n",
    "The Naive Bayes algorithm is a simple but powerful probabilistic classifier based on applying Bayes' theorem. This implementation builds the classifier **from scratch** in Python.\n",
    "\n",
    "### Key aspects:\n",
    "\n",
    "- **NaiveBayes class** contains methods for fitting the model from training data and predicting new samples.\n",
    "\n",
    "- fit() method calculates mean, variance, and class priors for each feature dimension per class. This learns the probability distributions. \n",
    "\n",
    "- predict() takes the log of posterior probabilities per class and returns the class with the highest probability via argmax(). \n",
    "\n",
    "- _predict() calculates posterior as the log of priors multiplied by log PDF (probability density function) of each feature per class.\n",
    "\n",
    "- PDF uses Gaussian distribution with learned means and variances as it assumes independence between features (naive assumption).\n",
    "\n",
    "- **Data** for evaluation is generated using sklearn make_classification() with 1000 samples, 2 classes and 10 features. \n",
    "\n",
    "- Train test split is done to evaluate held-out data.\n",
    "\n",
    "- Predictions are made and accuracy is calculated, showing ~96.5% accuracy validating the approach.\n",
    "\n",
    "This provides an intuitive understanding of the Naive Bayes algorithm and how each component like priors, posterior calculation, fitting, and predicting fits together. \n",
    "\n",
    "The from-scratch implementation without libraries helps cement the conceptual and implementation aspects of probabilistic classification. Proper evaluation establishes the viability of this simple but effective algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c8f269-0a22-4440-887f-55469ec4857c",
   "metadata": {},
   "source": [
    "## The fit method:\n",
    "\n",
    "This class function extracts statistics like mean, variance, and priors from the raw training data for each class-feature combination. These learned statistics are then used during prediction to apply the Naive Bayes assumptions. Here is a detailed line-by-line explanation of the fit method:\n",
    "\n",
    "\n",
    "def fit(self, X, y):\n",
    "- Fits the NB model to the training data X and labels y\n",
    "\n",
    "N, n = X.shape  \n",
    "- Gets the number of samples N and features n from X shape\n",
    "\n",
    "self.classes = np.unique(y)\n",
    "- Finds unique class labels present in y\n",
    "\n",
    "n_classes = len(self.classes)\n",
    "- Counts number of classes \n",
    "\n",
    "self.mean = np.zeros((n_classes, n))\n",
    "self.var = np.zeros((n_classes, n))\n",
    "- Initializes mean and variance arrays to store per-class stats\n",
    "\n",
    "self.priors = np.zeros(n_classes)\n",
    "- Initializes priors array \n",
    "\n",
    "for idx, c in enumerate(self.classes):\n",
    "- Loops through each class\n",
    "\n",
    "X_c = X[y==c]  \n",
    "- Filters X to only samples of class c\n",
    "\n",
    "self.mean[idx, :] = X_c.mean(axis=0)\n",
    "- Takes the mean of each feature in X_c and stores in the mean \n",
    "\n",
    "self.var[idx, :] = X_c.var(axis=0)  \n",
    "- Takes variance of each feature in X_c and stores in var\n",
    "\n",
    "self.priors[idx] = len(X_c) / N\n",
    "- Calculates prior as a fraction of samples of class c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9806978-c202-48df-bd9b-09154b2adc7b",
   "metadata": {},
   "source": [
    "## The prediction method:\n",
    "\n",
    "This class function provides a comprehensive understanding of how the Naive Bayes model makes probabilistic predictions by efficiently calculating posteriors class-by-class. Here's a detailed line-by-line explanation of the predict method:\n",
    "\n",
    "\n",
    "def predict(self, X):\n",
    "- This method takes the trained NaiveBayes model and new input data X to make predictions on.\n",
    "\n",
    "y_pred = [self._predict(x) for x in X]\n",
    "- A list is initialized to hold the predicted class labels for each sample in X. \n",
    "- The _predict helper method is called on each sample x to get its prediction.\n",
    "\n",
    "return np.array(y_pred)\n",
    "- The list of predictions is converted to a NumPy array and returned.\n",
    "\n",
    "def _predict(self, x):\n",
    "- Helper method that takes a single sample x and predicts its class.\n",
    "\n",
    "posteriors = []\n",
    "- An empty list is used to store the posterior probability calculated for each class.\n",
    "\n",
    "for idx, c in enumerate(self.classes):\n",
    "- Loops through each unique class learned during fit().\n",
    "\n",
    "prior = self.priors[idx]  \n",
    "- Extracts the prior probability of this class from what was stored during fit().\n",
    "\n",
    "posterior = np.sum(np.log(self._pdf(idx, x)))\n",
    "- Calls method to get log PDF values of x for this class and sums them  \n",
    "- Takes the log as posterior is the product of log priors and log PDFs\n",
    "\n",
    "posterior = posterior + prior\n",
    "- Adds the log prior to the log posterior \n",
    "\n",
    "posteriors.append(posterior)\n",
    "- Appends the posterior to the list\n",
    "\n",
    "return self.classes[np.argmax(posteriors)]\n",
    "- Returns the class label with the highest posterior probability via argmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
