{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c0b95c9-8720-4623-900b-abc0ca22342e",
   "metadata": {},
   "source": [
    "## Implementation of Naive Bayes Classifier from Scratch\n",
    "\n",
    "### Objective: \n",
    "\n",
    "Develop a Naive Bayes classifier for classification problems without using external ML libraries. This improves the understanding of probabilistic modeling approaches.\n",
    "\n",
    "#### Methodology:  \n",
    "\n",
    "- NaiveBayes class implements classifier functionality in an object-oriented manner.\n",
    "- The fit() method calculates class priors, mean, and variance to learn Gaussian distributions for each feature.  \n",
    "- The predict() method evaluates log posteriors by multiplying log priors and log PDF per class to return the predicted class label.\n",
    "- Data is generated using scikit-learn for evaluation on 1000 samples with 2 classes and 10 features.\n",
    "\n",
    "#### Evaluation:\n",
    "\n",
    "- Train-test split is done to validate on held-out data. \n",
    "- Accuracy metric is used with 96.5% achieved, showing algorithm effectiveness.\n",
    "\n",
    "#### Skills:\n",
    "\n",
    "- Probabilistic modeling concepts\n",
    "- Implementation of Bayes' theorem for classification  \n",
    "- NumPy for numeric computing\n",
    "- Data generation and preprocessing \n",
    "- Model evaluation best practices\n",
    "\n",
    "#### Outcome:\n",
    "- Intuitive and rigorous implementation establishes a solid understanding of probabilistic algorithms.  \n",
    "- Successful evaluation of simulated data proves the viability of the from-scratch approach.\n",
    "- Key probability concepts like density functions, priors, and posteriors are seamlessly integrated.\n",
    "\n",
    "\n",
    "In conclusion, this project demonstrates capabilities in algorithms, probabilistic modeling, and the ability to build classifiers from the ground up without libraries. A clean, modularized code structure and end-to-end validation approach are showcased. Developing basic machine learning techniques from first principles aids conceptual clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81feeca7-c2a0-4435-a4de-d24abdb12281",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74c9cf6f-5728-453e-b811-26b16c5bdd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297ec0a3-e60c-467c-b7d9-9440fd87bc99",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1318ae1-cc10-4690-99e1-d4b56b4d91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        N, n = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "\n",
    "        self.mean = np.zeros((n_classes, n), dtype=np.float64)\n",
    "        self.var = np.zeros((n_classes, n), dtype=np.float64)\n",
    "        self.priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[ y==c ]\n",
    "            self.mean[idx, :] = X_c.mean(axis=0)\n",
    "            self.var[idx, :] = X_c.var(axis=0)\n",
    "            self.priors[idx] = len(X_c) / N\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        posteriors = []\n",
    "        \n",
    "        for idx, c in enumerate(self.classes):\n",
    "            prior = self.priors[idx]\n",
    "            posterior = np.sum( np.log(self._pdf(idx, x)) )\n",
    "            posterior = posterior + prior\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "   \n",
    "    def _pdf(self, idx, x):\n",
    "        mean = self.mean[idx, :]\n",
    "        var = self.var[idx, :]\n",
    "\n",
    "        numerator = np.exp(-(x-mean)**2/(2*var**2))\n",
    "        denuminator = np.sqrt(2 * np.pi * var**2) \n",
    "        return numerator / denuminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f701cc1e-9a1f-4e48-a484-e06e81fc01da",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64089dd2-0eed-4df8-9c46-f56b0275477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe6eb5-8fcd-4033-8e37-a2b90d46f0c8",
   "metadata": {},
   "source": [
    "### Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07e9839-4ced-4e51-8a33-c94ed177cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cef497-8ca9-4ba5-9ea9-679bd085614b",
   "metadata": {},
   "source": [
    "### Training the KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07d7d4a8-ff5f-4336-89d1-6ba012f47173",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19f6e7-65e9-4166-a9d6-ce2dc39b019a",
   "metadata": {},
   "source": [
    "### Predicting the Test set & Evaluation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4049f610-f39c-412d-8ca3-62698fbf29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd8c255-e363-4125-a2d1-70666783f0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes classification accuracy 0.965\n"
     ]
    }
   ],
   "source": [
    " def accuracy(y_true, y_pred):\n",
    "     accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "     return accuracy\n",
    "\n",
    "print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
